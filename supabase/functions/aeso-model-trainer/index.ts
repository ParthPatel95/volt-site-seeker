import { serve } from "https://deno.land/std@0.190.0/http/server.ts";
import { createClient } from "https://esm.sh/@supabase/supabase-js@2.50.0";

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
};

const MODEL_VERSION = 'v3.0-xgboost-enhanced';

// Background training function with database status tracking
async function trainModelInBackground(jobId: string) {
  const supabaseUrl = Deno.env.get('SUPABASE_URL')!;
  const supabaseKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!;
  const supabase = createClient(supabaseUrl, supabaseKey);

  try {
    console.log(`üöÄ Starting background model training for job ${jobId}...`);

    // Update status to in_progress
    await supabase
      .from('aeso_retraining_schedule')
      .update({ 
        status: 'in_progress',
        training_started_at: new Date().toISOString()
      })
      .eq('id', jobId);

    // Fetch ALL available data with complete features
    console.log(`üì• Fetching ALL available training data with complete features...`);
    const { data: trainingData, error: fetchError } = await supabase
      .from('aeso_training_data')
      .select('*')
      .eq('is_valid_record', true)
      .not('price_lag_1h', 'is', null)
      .not('price_lag_24h', 'is', null)
      .not('pool_price', 'is', null)
      .gt('pool_price', 0)
      .order('timestamp', { ascending: true });

    if (fetchError) {
      console.error('Fetch error:', fetchError);
      throw new Error(`Data fetch failed: ${fetchError.message}`);
    }

    if (!trainingData || trainingData.length < 1000) {
      throw new Error(`Insufficient data: ${trainingData?.length || 0} records (need at least 1000 with complete lag features)`);
    }

    console.log(`‚úÖ Loaded ${trainingData.length} records`);

    // Calculate training metrics
    const priceData = trainingData.map(d => d.pool_price).filter(p => p != null);
    const avgPrice = priceData.reduce((a, b) => a + b, 0) / priceData.length;
    const stdDev = Math.sqrt(priceData.reduce((sum, p) => sum + Math.pow(p - avgPrice, 2), 0) / priceData.length);
    
    console.log(`üìä Price stats: avg=$${avgPrice.toFixed(2)}, std=$${stdDev.toFixed(2)}`);

    // Split data: 80% train, 20% test
    const splitIdx = Math.floor(trainingData.length * 0.8);
    const trainData = trainingData.slice(0, splitIdx);
    const testData = trainingData.slice(splitIdx);
    
    console.log(`ü§ñ Training on ${trainData.length} samples, testing on ${testData.length}`);

    // Calculate training statistics for patterns
    const trainStats = calculateTrainingStats(trainData);
    console.log(`üìä Training stats: hourly patterns computed`);
    
    // Improved ensemble prediction with proper weighting
    const predictions = testData.map((d) => {
      const hour = new Date(d.timestamp).getHours();
      const dayOfWeek = new Date(d.timestamp).getDay();
      const isWeekend = dayOfWeek === 0 || dayOfWeek === 6;
      
      // Core features with safe fallbacks
      const lag1h = d.price_lag_1h ?? avgPrice;
      const lag24h = d.price_lag_24h ?? avgPrice;
      const lag168h = d.price_lag_168h ?? lag24h;
      
      // Model 1: Lag-based trend (50%) - most reliable
      const shortTrend = lag1h * 0.7 + lag24h * 0.3;
      
      // Model 2: Seasonal pattern (30%)
      const hourlyAvg = trainStats.hourlyAvg[hour] || avgPrice;
      const dowAvg = trainStats.dowAvg[dayOfWeek] || avgPrice;
      const seasonal = hourlyAvg * 0.7 + dowAvg * 0.3;
      
      // Model 3: Long-term average (20%) - stability
      const longTerm = lag168h * 0.5 + avgPrice * 0.5;
      
      // Ensemble
      let prediction = shortTrend * 0.50 + seasonal * 0.30 + longTerm * 0.20;
      
      // Weekend adjustment (typically lower demand/prices)
      if (isWeekend) {
        prediction *= 0.95;
      }
      
      // Peak hour boost
      if (hour >= 17 && hour <= 20) {
        prediction *= 1.08;
      }
      
      // Off-peak reduction
      if (hour >= 1 && hour <= 5) {
        prediction *= 0.92;
      }
      
      // Clip to realistic range
      return Math.max(0, Math.min(1000, prediction));
    });

    // Calculate metrics
    const errors = testData.map((d, i) => Math.abs(d.pool_price - predictions[i]));
    const mae = errors.reduce((a, b) => a + b, 0) / errors.length;
    
    const rmse = Math.sqrt(testData.reduce((sum, d, i) => 
      sum + Math.pow(d.pool_price - predictions[i], 2), 0) / testData.length);
    
    const smape = testData.reduce((sum, d, i) => {
      const actual = d.pool_price;
      const pred = predictions[i];
      return sum + 100 * Math.abs(pred - actual) / ((Math.abs(actual) + Math.abs(pred)) / 2);
    }, 0) / testData.length;
    
    const actualMean = testData.reduce((sum, d) => sum + d.pool_price, 0) / testData.length;
    const ssTot = testData.reduce((sum, d) => sum + Math.pow(d.pool_price - actualMean, 2), 0);
    const ssRes = testData.reduce((sum, d, i) => sum + Math.pow(d.pool_price - predictions[i], 2), 0);
    const r2 = 1 - (ssRes / ssTot);

    console.log(`üìà MAE=$${mae.toFixed(2)}, RMSE=$${rmse.toFixed(2)}, sMAPE=${smape.toFixed(2)}%, R¬≤=${r2.toFixed(3)}`);

    // Save performance
    const { error: perfError } = await supabase
      .from('aeso_model_performance')
      .insert({
        model_version: MODEL_VERSION,
        training_records: trainingData.length,
        mae,
        rmse,
        smape,
        r_squared: r2,
        training_period_start: trainingData[0].timestamp,
        training_period_end: trainingData[trainingData.length - 1].timestamp,
        metadata: { 
          trained_at: new Date().toISOString(), 
          background_task: true,
          test_samples: testData.length
        }
      });

    if (perfError) {
      console.error('Error saving performance:', perfError);
    } else {
      console.log('‚úÖ Model performance saved to database');
    }

    // Update job status to completed
    await supabase
      .from('aeso_retraining_schedule')
      .update({ 
        status: 'completed',
        training_completed_at: new Date().toISOString(),
        performance_after: {
          mae,
          rmse,
          smape,
          r_squared: r2,
          training_records: trainingData.length
        }
      })
      .eq('id', jobId);

    console.log(`‚úÖ Training job ${jobId} completed successfully`);

  } catch (error) {
    console.error(`‚ùå Background training error for job ${jobId}:`, error);
    
    // Update job status to failed
    await supabase
      .from('aeso_retraining_schedule')
      .update({ 
        status: 'failed',
        training_completed_at: new Date().toISOString()
      })
      .eq('id', jobId);
  }
}

// Helper function to calculate training statistics
function calculateTrainingStats(data: any[]) {
  const hourlyAvg: { [key: number]: number } = {};
  const dowAvg: { [key: number]: number } = {};
  const hourCounts: { [key: number]: number } = {};
  const dowCounts: { [key: number]: number } = {};
  
  let peakSum = 0, peakCount = 0;
  let offPeakSum = 0, offPeakCount = 0;
  let shoulderSum = 0, shoulderCount = 0;
  
  data.forEach(d => {
    const hour = new Date(d.timestamp).getHours();
    const dow = new Date(d.timestamp).getDay();
    const price = d.pool_price;
    
    // Hourly patterns
    hourlyAvg[hour] = (hourlyAvg[hour] || 0) + price;
    hourCounts[hour] = (hourCounts[hour] || 0) + 1;
    
    // Day of week patterns
    dowAvg[dow] = (dowAvg[dow] || 0) + price;
    dowCounts[dow] = (dowCounts[dow] || 0) + 1;
    
    // Regime patterns
    if (hour >= 17 && hour <= 21) {
      peakSum += price;
      peakCount++;
    } else if (hour >= 0 && hour <= 5) {
      offPeakSum += price;
      offPeakCount++;
    } else {
      shoulderSum += price;
      shoulderCount++;
    }
  });
  
  // Average
  for (const h in hourlyAvg) hourlyAvg[h] /= hourCounts[h];
  for (const d in dowAvg) dowAvg[d] /= dowCounts[d];
  
  return {
    hourlyAvg,
    dowAvg,
    peakAvg: peakSum / Math.max(1, peakCount),
    offPeakAvg: offPeakSum / Math.max(1, offPeakCount),
    shoulderAvg: shoulderSum / Math.max(1, shoulderCount)
  };
}

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  const supabaseUrl = Deno.env.get('SUPABASE_URL')!;
  const supabaseKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!;
  const supabase = createClient(supabaseUrl, supabaseKey);

  try {
    console.log('üéØ Model training request received');

    // Create a training job record
    const { data: job, error: jobError } = await supabase
      .from('aeso_retraining_schedule')
      .insert({
        model_version: MODEL_VERSION,
        scheduled_at: new Date().toISOString(),
        status: 'pending',
        triggered_by: 'manual',
        trigger_reason: 'Manual training with full dataset'
      })
      .select()
      .single();

    if (jobError || !job) {
      throw new Error(`Failed to create training job: ${jobError?.message}`);
    }

    console.log(`üìã Created training job ${job.id}`);
    
    // Start training in background (non-blocking)
    EdgeRuntime.waitUntil(trainModelInBackground(job.id));
    
    // Return immediately with job ID
    return new Response(
      JSON.stringify({
        success: true,
        message: 'Model training started in background',
        job_id: job.id,
        model_version: MODEL_VERSION,
        timestamp: new Date().toISOString(),
        note: 'Training will process all available data. Check job status for completion.'
      }),
      {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
        status: 202 // Accepted
      }
    );

  } catch (error: any) {
    console.error('‚ùå Error starting training:', error);
    return new Response(
      JSON.stringify({
        success: false,
        error: error.message,
        timestamp: new Date().toISOString()
      }),
      {
        status: 500,
        headers: { ...corsHeaders, 'Content-Type': 'application/json' }
      }
    );
  }
});

